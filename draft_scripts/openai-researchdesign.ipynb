{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":101597,"databundleVersionId":12334818,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-31T19:09:24.262517Z","iopub.execute_input":"2025-05-31T19:09:24.262833Z","iopub.status.idle":"2025-05-31T19:09:26.890384Z","shell.execute_reply.started":"2025-05-31T19:09:24.262786Z","shell.execute_reply":"2025-05-31T19:09:26.889260Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Let's draw up a research plan- Guided by OpenAI**","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade openai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T19:10:32.789601Z","iopub.execute_input":"2025-05-31T19:10:32.790916Z","iopub.status.idle":"2025-05-31T19:10:41.174395Z","shell.execute_reply.started":"2025-05-31T19:10:32.790861Z","shell.execute_reply":"2025-05-31T19:10:41.172925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import openai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T19:11:44.175867Z","iopub.execute_input":"2025-05-31T19:11:44.176517Z","iopub.status.idle":"2025-05-31T19:11:45.616024Z","shell.execute_reply.started":"2025-05-31T19:11:44.176467Z","shell.execute_reply":"2025-05-31T19:11:45.615048Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"OpenAI\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T19:12:09.632818Z","iopub.execute_input":"2025-05-31T19:12:09.634063Z","iopub.status.idle":"2025-05-31T19:12:09.778473Z","shell.execute_reply.started":"2025-05-31T19:12:09.633915Z","shell.execute_reply":"2025-05-31T19:12:09.777320Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom openai import OpenAI\nfrom kaggle_secrets import UserSecretsClient\n\n# Access OpenAI key from Kaggle Secrets\nuser_secrets = UserSecretsClient()\nopenai_key = user_secrets.get_secret(\"OpenAI\")  # \"OpenAI\" should be the secret label\n\n# Create OpenAI client\nclient = OpenAI(api_key=openai_key)\n\n# Request a research plan\nchat_completion = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a world-class remote sensing archaeologist and research strategist.\"},\n        {\"role\": \"user\", \"content\": \"\"\"\nYou are participating in a global challenge to find unknown archaeological sites in the Amazon using satellite imagery, LIDAR, and historical documents.\n\nYour tools include:\n- High-resolution satellite imagery (Sentinel-2, Landsat, NICFI)\n- LIDAR data (1m–5m resolution) from OpenTopography\n- Colonial-era texts and Indigenous maps (to be parsed with GPT/NLP)\n- Access to geospatial libraries (rasterio, geopandas, openCV)\n- Python, Jupyter notebooks, Kaggle environment\n- Models: CNN, ViT, GPT-4, o4-mini\n\nThe geographic focus is the Santarém region in northern Brazil. You're expected to:\n1. Detect potential archaeological settlements hidden beneath forest canopy.\n2. Validate coordinates with two independent methods (e.g., LIDAR + text).\n3. Package your findings in a reproducible notebook + PDF report.\n\nNow, devise a **step-by-step research plan** including:\n- Data acquisition and bounding box\n- Data preprocessing and masking\n- Feature engineering and ML approach\n- Validation strategy\n- Expected output format\n- Risks and mitigation\n\nPlease format the output as markdown with numbered steps.\n\"\"\"}\n    ]\n)\n\n# Print the markdown-formatted research plan\nprint(chat_completion.choices[0].message.content)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T19:12:32.116434Z","iopub.execute_input":"2025-05-31T19:12:32.117044Z","iopub.status.idle":"2025-05-31T19:12:58.843192Z","shell.execute_reply.started":"2025-05-31T19:12:32.116982Z","shell.execute_reply":"2025-05-31T19:12:58.842074Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Open AI Supported Lit Review**","metadata":{}},{"cell_type":"code","source":"import requests\n\nquery = \"Amazon LiDAR archaeology Sentinel Landsat\"\nurl = f\"https://api.semanticscholar.org/graph/v1/paper/search?query={query}&limit=10&fields=title,authors,year,abstract,url\"\n\nresponse = requests.get(url)\ndata = response.json()\n\nfor i, paper in enumerate(data.get(\"data\", []), start=1):\n    print(f\"{i}. {paper['title']}\")\n    print(\"   Authors:\", \", \".join(a[\"name\"] for a in paper[\"authors\"]))\n    print(\"   Year:\", paper[\"year\"])\n    print(\"   URL:\", paper[\"url\"])\n    print()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T19:14:48.270166Z","iopub.execute_input":"2025-05-31T19:14:48.270630Z","iopub.status.idle":"2025-05-31T19:14:48.597994Z","shell.execute_reply.started":"2025-05-31T19:14:48.270599Z","shell.execute_reply":"2025-05-31T19:14:48.596954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\n\nquery = \"Amazon LiDAR archaeology Sentinel\"\nurl = f\"https://api.semanticscholar.org/graph/v1/paper/search?query={query}&limit=10&fields=title,authors,year,abstract,url\"\n\nresponse = requests.get(url)\ndata = response.json()\n\nfor i, paper in enumerate(data.get(\"data\", []), start=1):\n    print(f\"{i}. {paper['title']}\")\n    print(\"   Authors:\", \", \".join(a[\"name\"] for a in paper[\"authors\"]))\n    print(\"   Year:\", paper[\"year\"])\n    print(\"   URL:\", paper[\"url\"])\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T19:15:02.347150Z","iopub.execute_input":"2025-05-31T19:15:02.347597Z","iopub.status.idle":"2025-05-31T19:15:12.638870Z","shell.execute_reply.started":"2025-05-31T19:15:02.347562Z","shell.execute_reply":"2025-05-31T19:15:12.637177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query = \"Amazon archaeology NDVI\"\nurl = f\"https://api.semanticscholar.org/graph/v1/paper/search?query={query}&limit=10&fields=title,authors,year,abstract,url\"\n\nresponse = requests.get(url)\ndata = response.json()\n\nfor i, paper in enumerate(data.get(\"data\", []), start=1):\n    print(f\"{i}. {paper['title']}\")\n    print(\"   Authors:\", \", \".join(a[\"name\"] for a in paper[\"authors\"]))\n    print(\"   Year:\", paper[\"year\"])\n    print(\"   URL:\", paper[\"url\"])\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T19:15:19.633543Z","iopub.execute_input":"2025-05-31T19:15:19.633926Z","iopub.status.idle":"2025-05-31T19:15:20.236000Z","shell.execute_reply.started":"2025-05-31T19:15:19.633900Z","shell.execute_reply":"2025-05-31T19:15:20.234785Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Other Literature","metadata":{}},{"cell_type":"code","source":"import requests\nimport pandas as pd\nfrom IPython.display import display, HTML\n\n# List of external DOIs\ndois = [\n    \"10.7717/peerj.15137\",                   # PeerJ\n    \"10.1038/s41467-018-03510-7\"             # Nature Communications\n]\n\nrecords = []\n\nfor doi in dois:\n    url = f\"https://api.crossref.org/works/{doi}\"\n    r = requests.get(url)\n\n    if r.status_code == 200:\n        data = r.json()[\"message\"]\n        title = data.get(\"title\", [\"N/A\"])[0]\n        authors = \", \".join([f\"{a.get('given', '')} {a.get('family', '')}\".strip() for a in data.get(\"author\", [])])\n        year = data.get(\"published-print\", data.get(\"published-online\", {})).get(\"date-parts\", [[None]])[0][0]\n        abstract = data.get(\"abstract\", \"N/A\").replace(\"<jats:p>\", \"\").replace(\"</jats:p>\", \"\") if \"abstract\" in data else \"N/A\"\n\n        records.append({\n            \"Title\": title,\n            \"Year\": year,\n            \"DOI\": f\"<a href='https://doi.org/{doi}' target='_blank'>{doi}</a>\",\n            \"Abstract\": abstract,\n            \"Link\": f\"<a href='https://doi.org/{doi}' target='_blank'>View Paper</a>\",\n            \"Origin\": \"CrossRef\"\n        })\n    else:\n        records.append({\n            \"Title\": \"N/A\",\n            \"Year\": \"N/A\",\n            \"DOI\": doi,\n            \"Abstract\": \"Could not fetch from CrossRef\",\n            \"Link\": f\"<a href='https://doi.org/{doi}' target='_blank'>Link</a>\",\n            \"Origin\": \"CrossRef\"\n        })\n\n# Show combined table\ndf = pd.DataFrame(records)\npd.set_option(\"display.max_colwidth\", None)\ndisplay(HTML(df.to_html(escape=False, index=False)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T19:15:53.093825Z","iopub.execute_input":"2025-05-31T19:15:53.094819Z","iopub.status.idle":"2025-05-31T19:15:53.708448Z","shell.execute_reply.started":"2025-05-31T19:15:53.094774Z","shell.execute_reply":"2025-05-31T19:15:53.707323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\nimport pandas as pd\nfrom IPython.display import display, HTML\n\n# === Step 1: Load Semantic Scholar papers into df_ss ===\npaper_ids = [\n    \"a57a33a75366fcc98ee60167ef5e909e7e01236f\",\n    \"a57a31b8ac9e0d93751374f8e39ef7bb64f362ca\",\n    \"d389b863c924b0a755745bc43d32af7ff16cb532\",\n    \"d9472c92bb31a001c2dbce9a4a25f9bac6be3640\",\n    \"369c2329587b7fc73ac84766ff7dd1ea9abf0816\",\n    \"b70459ec0536d9747b5f00e219daf52ffac8982f\",\n    \"51ea9a2b69c3e48db02da782dbd3b76359428274\",\n    \"5cb1290372394009b9225b010debea5e5c26c2cc\",\n    \"09f3b85cd7fdc9dd3b9f25a55295b5e3d9cad17d\",\n    \"5887a1cdb74d149f4d6f9a5160cff45e6141e7ff\",\n    \"a69b0b75368b3f32136042b8b8341b133f7b7bcf\",\n    \"8dde685ff036b7027fc23fd71311692105de1e01\",\n    \"afae001fc095865ebff60fd6e1f11d4fc1362e2c\"\n]\n\nrecords = []\nfor pid in paper_ids:\n    if not pid.strip():\n        continue\n\n    url = f\"https://api.semanticscholar.org/graph/v1/paper/{pid}?fields=title,year,abstract,url,externalIds\"\n    r = requests.get(url)\n    if r.status_code == 200:\n        data = r.json()\n        doi = data.get(\"externalIds\", {}).get(\"DOI\", \"N/A\")\n        doi_link = f\"<a href='https://doi.org/{doi}' target='_blank'>{doi}</a>\" if doi != \"N/A\" else \"N/A\"\n        records.append({\n            \"Title\": data.get(\"title\", \"N/A\"),\n            \"Year\": data.get(\"year\", \"N/A\"),\n            \"DOI\": doi_link,\n            \"Abstract\": data.get(\"abstract\", \"N/A\"),\n            \"Link\": f\"<a href='{data.get('url')}' target='_blank'>View Paper</a>\",\n            \"Source\": \"Semantic Scholar\"\n        })\n    else:\n        records.append({\n            \"Title\": \"N/A\",\n            \"Year\": \"N/A\",\n            \"DOI\": \"N/A\",\n            \"Abstract\": \"Failed to fetch\",\n            \"Link\": f\"<a href='https://www.semanticscholar.org/paper/{pid}' target='_blank'>Link</a>\",\n            \"Source\": \"Semantic Scholar\"\n        })\n\ndf_ss = pd.DataFrame(records)\n\n# === Step 2: Fetch additional CrossRef papers ===\ncrossref_dois = [\n    \"10.7717/peerj.15137\",\n    \"10.1038/s41467-018-03510-7\"\n]\n\ncrossref_records = []\n\nfor doi in crossref_dois:\n    url = f\"https://api.crossref.org/works/{doi}\"\n    r = requests.get(url)\n\n    if r.status_code == 200:\n        data = r.json()[\"message\"]\n        title = data.get(\"title\", [\"N/A\"])[0]\n        authors = \", \".join([f\"{a.get('given', '')} {a.get('family', '')}\".strip() for a in data.get(\"author\", [])])\n        year = data.get(\"published-print\", data.get(\"published-online\", {})).get(\"date-parts\", [[None]])[0][0]\n        abstract = data.get(\"abstract\", \"N/A\").replace(\"<jats:p>\", \"\").replace(\"</jats:p>\", \"\") if \"abstract\" in data else \"N/A\"\n        doi_link = f\"<a href='https://doi.org/{doi}' target='_blank'>{doi}</a>\"\n\n        crossref_records.append({\n            \"Title\": title,\n            \"Year\": year,\n            \"DOI\": doi_link,\n            \"Abstract\": abstract,\n            \"Link\": f\"<a href='https://doi.org/{doi}' target='_blank'>View Paper</a>\",\n            \"Source\": \"CrossRef\"\n        })\n\ndf_crossref = pd.DataFrame(crossref_records)\n\n# === Step 3: Merge both\ndf_combined = pd.concat([df_ss, df_crossref], ignore_index=True)\n\n# === Step 4: Display\npd.set_option(\"display.max_colwidth\", None)\ndisplay(HTML(df_combined.to_html(escape=False, index=False)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T19:16:16.052867Z","iopub.execute_input":"2025-05-31T19:16:16.053314Z","iopub.status.idle":"2025-05-31T19:16:17.671650Z","shell.execute_reply.started":"2025-05-31T19:16:16.053282Z","shell.execute_reply":"2025-05-31T19:16:17.670191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}